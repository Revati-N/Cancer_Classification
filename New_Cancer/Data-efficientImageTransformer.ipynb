{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7ae012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reorganizing cancer data into binary classification structure...\n",
      "\n",
      "Created binary structure at: E:\\LY Project\\Multi Cancer\\Data_Binary\n",
      "  - E:\\LY Project\\Multi Cancer\\Data_Binary\\Malignant\n",
      "  - E:\\LY Project\\Multi Cancer\\Data_Binary\\Benign\n",
      "\n",
      "============================================================\n",
      "COPYING MALIGNANT IMAGES\n",
      "============================================================\n",
      "Processing Malignant: all_early - 5000 images\n",
      "Processing Malignant: all_pre - 5000 images\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 82\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCOPYING MALIGNANT IMAGES\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m---> 82\u001b[0m malignant_total \u001b[38;5;241m=\u001b[39m \u001b[43mcopy_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmalignant_sources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmalignant_dest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMalignant\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Copy benign images\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 73\u001b[0m, in \u001b[0;36mcopy_images\u001b[1;34m(source_folders, destination_folder, label)\u001b[0m\n\u001b[0;32m     70\u001b[0m         dest_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(destination_folder, new_filename)\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;66;03m# Copy file\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m         \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy2\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m         total_copied \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_copied\n",
      "File \u001b[1;32mc:\\Users\\kashu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py:434\u001b[0m, in \u001b[0;36mcopy2\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(dst):\n\u001b[0;32m    433\u001b[0m     dst \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dst, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(src))\n\u001b[1;32m--> 434\u001b[0m \u001b[43mcopyfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    435\u001b[0m copystat(src, dst, follow_symlinks\u001b[38;5;241m=\u001b[39mfollow_symlinks)\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
      "File \u001b[1;32mc:\\Users\\kashu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py:258\u001b[0m, in \u001b[0;36mcopyfile\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(dst, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fdst:\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;66;03m# macOS\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _HAS_FCOPYFILE:\n\u001b[0;32m    259\u001b[0m             \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    260\u001b[0m                 _fastcopy_fcopyfile(fsrc, fdst, posix\u001b[38;5;241m.\u001b[39m_COPYFILE_DATA)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# -------------------------------\n",
    "# PARAMETERS\n",
    "# -------------------------------\n",
    "img_height, img_width = 224, 224\n",
    "batch_size = 32\n",
    "num_classes = 2  # Binary classification\n",
    "epochs = 20\n",
    "\n",
    "# DeiT Architecture Parameters\n",
    "patch_size = 16\n",
    "num_patches = (img_height // patch_size) ** 2\n",
    "projection_dim = 192  # DeiT-Tiny\n",
    "num_heads = 3\n",
    "transformer_layers = 12\n",
    "mlp_head_units = [2048, 1024]\n",
    "dropout_rate = 0.0\n",
    "drop_path_rate = 0.1\n",
    "\n",
    "# -------------------------------\n",
    "# DEFINE DIRECTORIES\n",
    "# -------------------------------\n",
    "malignant_dirs = [\n",
    "    r\"E:\\LY Project\\Multi Cancer\\Data\\Malignant\\all_early\",\n",
    "    r\"E:\\LY Project\\Multi Cancer\\Data\\Malignant\\all_pre\",\n",
    "    r\"E:\\LY Project\\Multi Cancer\\Data\\Malignant\\all_pro\",\n",
    "    r\"E:\\LY Project\\Multi Cancer\\Data\\Malignant\\breast_malignant\",\n",
    "    r\"E:\\LY Project\\Multi Cancer\\Data\\Malignant\\colon_aca\",\n",
    "    r\"E:\\LY Project\\Multi Cancer\\Data\\Malignant\\lung_aca\",\n",
    "    r\"E:\\LY Project\\Multi Cancer\\Data\\Malignant\\lung_scc\",\n",
    "    r\"E:\\LY Project\\Multi Cancer\\Data\\Malignant\\oral_scc\"\n",
    "]\n",
    "\n",
    "benign_dirs = [\n",
    "    r\"E:\\LY Project\\Multi Cancer\\Data\\Benign\\all_benign\",\n",
    "    r\"E:\\LY Project\\Multi Cancer\\Data\\Benign\\breast_benign\",\n",
    "    r\"E:\\LY Project\\Multi Cancer\\Data\\Benign\\colon_bnt\",\n",
    "    r\"E:\\LY Project\\Multi Cancer\\Data\\Benign\\lung_bnt\",\n",
    "    r\"E:\\LY Project\\Multi Cancer\\Data\\Benign\\oral_normal\"\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# LOAD DATA FROM MULTIPLE DIRECTORIES\n",
    "# -------------------------------\n",
    "print(\"Loading data from multiple directories...\")\n",
    "\n",
    "def load_images_from_directories(directories, label, img_height, img_width):\n",
    "    \"\"\"Load images from multiple directories and assign same label\"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for directory in directories:\n",
    "        if not os.path.exists(directory):\n",
    "            print(f\"âš  Warning: Directory not found - {directory}\")\n",
    "            continue\n",
    "        \n",
    "        # Get all image files\n",
    "        files = [os.path.join(directory, f) for f in os.listdir(directory)\n",
    "                if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif'))]\n",
    "        \n",
    "        folder_name = os.path.basename(directory)\n",
    "        print(f\"  {folder_name}: {len(files)} images\")\n",
    "        \n",
    "        image_paths.extend(files)\n",
    "        labels.extend([label] * len(files))\n",
    "    \n",
    "    return image_paths, labels\n",
    "\n",
    "# Load malignant images (label = 1)\n",
    "print(\"\\nLoading MALIGNANT images:\")\n",
    "malignant_paths, malignant_labels = load_images_from_directories(\n",
    "    malignant_dirs, label=1, img_height=img_height, img_width=img_width\n",
    ")\n",
    "\n",
    "# Load benign images (label = 0)\n",
    "print(\"\\nLoading BENIGN images:\")\n",
    "benign_paths, benign_labels = load_images_from_directories(\n",
    "    benign_dirs, label=0, img_height=img_height, img_width=img_width\n",
    ")\n",
    "\n",
    "# Combine all data\n",
    "all_image_paths = malignant_paths + benign_paths\n",
    "all_labels = malignant_labels + benign_labels\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Total images loaded: {len(all_image_paths)}\")\n",
    "print(f\"  Malignant: {len(malignant_paths)} ({100*len(malignant_paths)/len(all_image_paths):.1f}%)\")\n",
    "print(f\"  Benign: {len(benign_paths)} ({100*len(benign_paths)/len(all_image_paths):.1f}%)\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# -------------------------------\n",
    "# CREATE TF.DATA.DATASET\n",
    "# -------------------------------\n",
    "def load_and_preprocess_image(path, label):\n",
    "    \"\"\"Load and preprocess a single image\"\"\"\n",
    "    # Read image file\n",
    "    image = tf.io.read_file(path)\n",
    "    # Decode image\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    # Resize\n",
    "    image = tf.image.resize(image, [img_height, img_width])\n",
    "    # Normalize to [0, 1]\n",
    "    image = image / 255.0\n",
    "    # Convert label to categorical\n",
    "    label = tf.one_hot(label, depth=num_classes)\n",
    "    return image, label\n",
    "\n",
    "# Convert to TensorFlow dataset\n",
    "path_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_labels))\n",
    "\n",
    "# Shuffle all data\n",
    "path_ds = path_ds.shuffle(buffer_size=len(all_image_paths), seed=42)\n",
    "\n",
    "# Split into train and validation (80-20 split)\n",
    "train_size = int(0.8 * len(all_image_paths))\n",
    "val_size = len(all_image_paths) - train_size\n",
    "\n",
    "train_ds = path_ds.take(train_size)\n",
    "val_ds = path_ds.skip(train_size)\n",
    "\n",
    "print(f\"Training samples: {train_size}\")\n",
    "print(f\"Validation samples: {val_size}\\n\")\n",
    "\n",
    "# Map loading and preprocessing function\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.map(load_and_preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "val_ds = val_ds.map(load_and_preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "val_ds = val_ds.batch(batch_size)\n",
    "val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# -------------------------------\n",
    "# DEIT ARCHITECTURE COMPONENTS\n",
    "# -------------------------------\n",
    "\n",
    "class PatchExtractor(layers.Layer):\n",
    "    \"\"\"Extracts patches from images\"\"\"\n",
    "    def __init__(self, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    \"\"\"Encodes patches with linear projection and positional embeddings\"\"\"\n",
    "    def __init__(self, num_patches, projection_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "class ClassTokenLayer(layers.Layer):\n",
    "    \"\"\"Adds a learnable class token to the sequence\"\"\"\n",
    "    def __init__(self, projection_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.projection_dim = projection_dim\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.class_token = self.add_weight(\n",
    "            name=\"class_token\",\n",
    "            shape=(1, 1, self.projection_dim),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def call(self, encoded_patches):\n",
    "        batch_size = tf.shape(encoded_patches)[0]\n",
    "        class_tokens = tf.broadcast_to(\n",
    "            self.class_token, [batch_size, 1, self.projection_dim]\n",
    "        )\n",
    "        return tf.concat([class_tokens, encoded_patches], axis=1)\n",
    "\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    \"\"\"Multi-layer perceptron block\"\"\"\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "class DropPath(layers.Layer):\n",
    "    \"\"\"Stochastic Depth / Drop Path regularization\"\"\"\n",
    "    def __init__(self, drop_prob=0.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if self.drop_prob == 0.0 or not training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (tf.shape(x)[0],) + (1,) * (len(x.shape) - 1)\n",
    "        random_tensor = keep_prob + tf.random.uniform(shape, dtype=x.dtype)\n",
    "        binary_tensor = tf.floor(random_tensor)\n",
    "        output = tf.divide(x, keep_prob) * binary_tensor\n",
    "        return output\n",
    "\n",
    "def create_deit_classifier(\n",
    "    input_shape,\n",
    "    patch_size,\n",
    "    num_patches,\n",
    "    projection_dim,\n",
    "    num_heads,\n",
    "    transformer_layers,\n",
    "    mlp_head_units,\n",
    "    num_classes,\n",
    "    dropout_rate=0.0,\n",
    "    drop_path_rate=0.1,\n",
    "):\n",
    "    \"\"\"Creates a Data-efficient Image Transformer (DeiT) model\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Create patches\n",
    "    patches = PatchExtractor(patch_size)(inputs)\n",
    "    \n",
    "    # Encode patches\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "    \n",
    "    # Add class token\n",
    "    encoded_patches = ClassTokenLayer(projection_dim)(encoded_patches)\n",
    "    \n",
    "    # Transformer blocks with stochastic depth\n",
    "    dpr = [x for x in np.linspace(0.0, drop_path_rate, transformer_layers)]\n",
    "    \n",
    "    for i in range(transformer_layers):\n",
    "        # Layer normalization 1\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        \n",
    "        # Multi-head self-attention\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=projection_dim // num_heads,\n",
    "            dropout=dropout_rate,\n",
    "        )(x1, x1)\n",
    "        \n",
    "        # Drop path\n",
    "        attention_output = DropPath(dpr[i])(attention_output)\n",
    "        \n",
    "        # Skip connection 1\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        \n",
    "        # Layer normalization 2\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        \n",
    "        # MLP\n",
    "        mlp_output = mlp(\n",
    "            x3,\n",
    "            hidden_units=[projection_dim * 4, projection_dim],\n",
    "            dropout_rate=dropout_rate,\n",
    "        )\n",
    "        \n",
    "        # Drop path\n",
    "        mlp_output = DropPath(dpr[i])(mlp_output)\n",
    "        \n",
    "        # Skip connection 2\n",
    "        encoded_patches = layers.Add()([mlp_output, x2])\n",
    "    \n",
    "    # Layer normalization\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    \n",
    "    # Extract class token representation\n",
    "    representation = layers.Lambda(lambda x: x[:, 0])(representation)\n",
    "    \n",
    "    # Classification head\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    logits = layers.Dense(num_classes, activation='softmax')(features)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model\n",
    "\n",
    "# -------------------------------\n",
    "# BUILD DEIT MODEL\n",
    "# -------------------------------\n",
    "print(\"Building Data-efficient Image Transformer (DeiT) model...\")\n",
    "\n",
    "model = create_deit_classifier(\n",
    "    input_shape=(img_height, img_width, 3),\n",
    "    patch_size=patch_size,\n",
    "    num_patches=num_patches,\n",
    "    projection_dim=projection_dim,\n",
    "    num_heads=num_heads,\n",
    "    transformer_layers=transformer_layers,\n",
    "    mlp_head_units=mlp_head_units,\n",
    "    num_classes=num_classes,\n",
    "    dropout_rate=dropout_rate,\n",
    "    drop_path_rate=drop_path_rate,\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# -------------------------------\n",
    "# COMPILE MODEL WITH ADAMW\n",
    "# -------------------------------\n",
    "print(\"\\nCompiling model with AdamW optimizer...\")\n",
    "\n",
    "optimizer = AdamW(\n",
    "    learning_rate=0.0005,\n",
    "    weight_decay=0.0001\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# CALLBACKS\n",
    "# -------------------------------\n",
    "checkpoint_filepath = 'best_deit_cancer_model.weights.h5'\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# TRAIN MODEL\n",
    "# -------------------------------\n",
    "print(f\"\\nTraining DeiT for {epochs} epochs...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training completed.\")\n",
    "print(f\"Best model saved at {checkpoint_filepath}\")\n",
    "\n",
    "# -------------------------------\n",
    "# EVALUATION AND METRICS\n",
    "# -------------------------------\n",
    "print(\"\\nLoading best model for evaluation...\")\n",
    "model.load_weights(checkpoint_filepath)\n",
    "\n",
    "print(\"Plotting training history...\")\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
    "plt.title('Model Accuracy Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss', marker='o')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', marker='s')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('deit_training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# PREDICTIONS AND EVALUATION\n",
    "# -------------------------------\n",
    "print(\"\\nPredicting classes on validation data...\")\n",
    "\n",
    "# Collect predictions and true labels\n",
    "y_pred_probs = []\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
